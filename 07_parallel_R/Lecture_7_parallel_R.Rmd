---
title: 'Parallel R'
subtitle: 'parXXapply, foreach/parallel, Rmpi'
author: "Bob Settlage"
date: '`r Sys.Date()`'
output:
  ioslides_presentation:
    fig_caption: yes
    fig_height: 6
    fig_width: 4
    smaller: yes
  slidy_presentation: default
---

```{r misc_function, eval=T, echo=F, warnings=F}
knitr::opts_chunk$set(echo = F, eval=T, cache=T, tidy.opts=list(width.cutoff=53),
                tidy=T, include=T, message=F, warning=F)
library(ggplot2)
library(ggExtra)
library(MASS)
##this posted answer rocks!!
##https://stackoverflow.com/questions/11022675/rotate-histogram-in-r-or-overlay-a-density-in-a-barplot

scatterBarNorm <- function(x, dcol="blue", lhist=20, num.dnorm=5*lhist, ...){
    ## check input
    stopifnot(ncol(x)==2)
    ## set up layout and graphical parameters
    layMat <- matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
    layout(layMat, widths=c(5/7, 2/7), heights=c(2/7, 5/7))
    ospc <- 0.5 # outer space
    pext <- 4 # par extension down and to the left
    bspc <- 1 # space between scatter plot and bar plots
    par. <- par(mar=c(pext, pext, bspc, bspc),
                oma=rep(ospc, 4)) # plot parameters
    ## scatter plot
    plot(x, xlim=range(x[,1]), ylim=range(x[,2]), pch=20, ...)
    ## 3) determine barplot and height parameter
    ## histogram (for barplot-ting the density)
    xhist <- hist(x[,1], plot=FALSE, breaks=seq(from=min(x[,1]),
                to=max(x[,1]),  length.out=lhist))
    yhist <- hist(x[,2], plot=FALSE, breaks=seq(from=min(x[,2]),
                to=max(x[,2]),  length.out=lhist)) # note: this uses probability=TRUE
    ## determine the plot range and all the things needed for the barplots and lines
    xx <- seq(min(x[,1]), max(x[,1]), length.out=num.dnorm) # evaluation points for the overlaid density
    xy <- dnorm(xx, mean=mean(x[,1]), sd=sd(x[,1])) # density points
    yx <- seq(min(x[,2]), max(x[,2]), length.out=num.dnorm)
    yy <- dnorm(yx, mean=mean(x[,2]), sd=sd(x[,2]))
    ## barplot and line for x (top)
    par(mar=c(0, pext, 0, 0))
    barplot(xhist$density, axes=FALSE, ylim=c(0, max(xhist$density, xy)),
            space=0, col = "grey") # barplot
    lines(seq(from=0, to=lhist-1, length.out=num.dnorm), xy, col=dcol) # line
    ## barplot and line for y (right)
    par(mar=c(pext, 0, 0, 0))
    barplot(yhist$density, axes=FALSE, xlim=c(0, max(yhist$density, yy)),
            space=0, horiz=TRUE, col = "orange") # barplot
    lines(yy, seq(from=0, to=lhist-1, length.out=num.dnorm), col=dcol) # line
    ## restore parameters
    par(par.)
}

```

## Today's Agenda

- Review vector and matrix math  
- Review the apply family of functions  
- Discuss methods to make things faster  
    + foreach
    + doParallel (SNOW-multicore)
    + parxxApply
    + Rmpi
- Homework 7

## vectors and matrices in R  

Super easy to create:  

```{r echo=T, eval=F, include=T}
    a <- c(1:5)
    B <- matrix(1:10,ncol=2)
    a
    B
```

## basic operations

* add, multiply vector/matrix by a scalar
* add, multiply vectors/matrix

And all the vector/matrix operations we would expect:  

* transpose of vector/matrix
* multiply vector/matrix
* some special operations
    + diag (2 forms)
    + solve
    + trace
    + determinant
    + Kronecker product


## Using duality to do stuff

colMeans.  What if we want the means of a matrix by columns.  Of course there are functions for this, but, for kicks, can we do this with some mixed matrix art ... ??

```{r eval=F, echo=T, include=T}

colMeans(C)
ones <- rep(1,3)
t(ones) %*% (C / 3)
```

## Making things faster:

A lot of effort has gone into making matrix math faster on computers.  Hopefully this sparks your curiousity enough to hit google:

```{r echo=T, eval=F, include=T, tidy=FALSE}

    A = matrix(rnorm(20*40000, mean=0, sd=5), 20, 40000)
    B = matrix(rnorm(20*40000, mean=0, sd=5), 20, 40000)
    system.time({t(A)%*%B})
    system.time({crossprod(A,B)})

```

Don't invert matrices:

<https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/>
<https://www.r-bloggers.com/dont-invert-that-matrix-why-and-how/>

```{r echo=T, eval=F, include=T, tidy=FALSE}

    n <- 5000
    A = matrix(rnorm(n*n, mean=0, sd=5), n, n)
    x <- rnorm(n)
    system.time({b_inverting <- solve(A)%*%x})
    system.time({b_solving <- solve(A,x)})

    max(abs(b_inverting - b_solving))
```

## Apply family of functions

We often want to "apply" a function along a "margin" of our data.  In the previous example, we used a function to loop through observers to calculate summary statistics.

In R, we have helper functions to further simplify our code by obviating the for loop.

Apply family:

apply, lapply , sapply, vapply, mapply, rapply, and tapply

Nice tutorial:  
<https://www.r-bloggers.com/r-tutorial-on-the-apply-family-of-functions/>

## Apply detail

*apply(X, MARGIN, FUN, ...)*

```{r echo=T, eval=F, include=T}
    # ?apply
    x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
    dimnames(x)[[1]] <- letters[1:8]
    x
    apply(x, 2, mean, trim = .2)
    col.sums <- apply(x, 2, sum)
    row.sums <- apply(x, 1, sum)
    
    coinToss<-function(x){rbinom(10,1,x)}
    sapply(c(30:40)/100,FUN=coinToss)

```

## What is difference between various apply functions

We could start with the help `?apply, ?sapply`. The main differences are:  
 <http://www.dummies.com/programming/r/how-to-use-the-apply-family-of-functions-in-r/>
 
## Apply functions

```{r apply_descriptions, eval=T, echo=F, include=T, results='asis'}

    library(rvest)
    library(kableExtra)
    webpage <- read_html("http://www.dummies.com/programming/r/how-to-use-the-apply-family-of-functions-in-r/")
    tbls <- html_nodes(webpage, "table")
    tbls_ls <- webpage %>%
            html_nodes("table") %>%
            #.[c(2:5,16:18)] %>%
            html_table(fill = TRUE)
    tbls_ls <- tbls_ls[[1]]
    knitr::kable(tbls_ls) %>% 
        kable_styling(font_size=5)
```

## Summary

We have in our tool belt: 

* for loops  
* matrix ops  
* apply functions  

Now we need methods to make things faster.  We discussed some methods around matrix operations.  What if we can't (easily) convert to matrix operations?

## Example

```{r eval=F, echo=T, include=T,tidy=F}
    
    set.seed(1245)
    n <- 30; B <- 100
    data <- 6 + rt(n, df = 1)
    tans <- rep(NA,B)
    system.time({
    for(b in 1:B){
        tans[b] <- sum(atan(1:5e6))
    }
    })
    calc_centers <- function(){
        sum(atan(1:5e6))
    }
    system.time({
    tans <- sapply(1:B,function(n) calc_centers(),
                      USE.NAMES = F)
    })

```

## foreach

How do we make this faster?  Crank up B and take a look at the useage of the various cores in the computer...

Perhaps we need to use more of our computer.

First, the for loop...

```{r echo=T, eval=F, include=T}

    library(foreach)
    system.time({
    tans <- foreach(b = 1:B, .combine='c') %do% {
        sum(atan(1:5e6))
    }
    })
```

## foreach

```{r echo=T, eval=F, include=T}

    library(foreach)
    library(doParallel)
    system.time({
    tans <- foreach(b = 1:B, .combine='c') %dopar% {
        sum(atan(1:5e6))
    }
    })
    #cl <- makeCluster(ncores=6)
    registerDoParallel(cores=6)
    system.time({
    tans <- foreach(b = 1:B, .combine='c') %dopar% {
        sum(atan(1:5e6))
    }
    })
    stopImplicitCluster()
    #stopCluster(cl)
    
```

## parXXapply

Ok, so we can do some fun stuff with for loops, what about parallelizing the apply functions??

```{r eval=F, echo=T, include=T,tidy=F}
    
    #library(doParallel)
    library(parallel)
    set.seed(1245)
    n <- 30; B <- 100
    data <- 6 + rt(n, df = 1)
    tans <- rep(NA,B)
    system.time({
    for(b in 1:B){
        tans[b] <- sum(atan(1:5e6))
    }
    })
    calc_centers <- function(){
        sum(atan(1:5e6))
    }
    
    cl <- makeCluster(6)
    clusterExport(cl,"calc_centers")
    system.time({
        tans <- parSapply(cl, 1:B,
                function(n) calc_centers(),USE.NAMES = F)
    })
    stopCluster(cl)

```

## Quick reminder on RNG

Remember, we often need random numbers.  We need them different on different workers and we need them reproducible.

```{r echo=T, eval=F, include=T, tidy=F}
    library(foreach)
    library(doParallel)
    library("doRNG")
    cl <- makeCluster(2); registerDoParallel(cl)
    
    set.seed(123)
    res <- foreach(i=1:5) %dopar% { runif(3) } 
    set.seed(123)
    res2 <- foreach(i=1:5) %dopar% { runif(3) } 
    identical(res, res2)
    
    registerDoRNG()
    set.seed(123)
    res <- foreach(i=1:5) %dopar% { runif(3) } 
    set.seed(123)
    res2 <- foreach(i=1:5) %dopar% { runif(3) } 
    identical(res, res2)
    
    stopCluster(cl)
```

## Regroup

What have we done?

Essentially, we have created a small cluster and run the same command across the cluster (SNOW).  What if we need to update the progress of one "worker" to other workers??

<http://www.glennklockwood.com/data-intensive/r/lapply-parallelism.html>
<https://portal.tacc.utexas.edu/documents/13601/901835/Parallel_R_Final.pdf/eaa55f54-3e3a-4a96-be95-1f4de44fa119>

## MPI

```{r,out.height="500px", out.width="750px", eval=T}
    knitr::include_graphics("MPI_slide.png",auto_pdf=F)
```

## MPI: Program Models

- "Brute Force": Decompose problem
- "Task Push": Master creates list of tasks and sends to slaves in round-robin fashion
- "Task Pull": Slaves report to master when finished, receive new tasks

Examples:

<http://cran.r-project.org/web/packages/pbdMPI/vigneIes/pbdMPI-guide.pdf>

## Rmpi

- User-developed package
- Interfce to MPI for R
    +Master/slave paradigm
- Allows parallelism beyond embarrassingly parallel, e.g. SNOW
- Provided as part of ARC R module


## Rmpi: Starting and Stopping

- Load library:
    library(Rmpi)
- Spawn nsl slaves
    mpi.spawn.Rslaves(nslaves=nsl)
- Shut down slaves (IMPORTANT)
    mpi.close.Rslaves()
- Clean up and quit R
    mpi.quit()
    
## Rmpi basics

- Run an Rmpi script like any other R script:
    Rscript mcpi_rmpi.r
- Get the number of processes (the number of slaves +1)
    mpi.comm.size()
- Get the rank of a process:
    mpi.comm.rank()
        + Mater: 0
        - Slave: 1+
        
## Rmpi: Executing Remotely

```{r eval=F, echo=T}

#Execute on the master:
paste("I am",mpi.comm.rank(),"of",mpi.comm.size())
   [1] "I am 0 of 3"

#Execute Rcommand on the slaves: 
mpi.bcast.cmd(Rcommand)

#Execute on the slaves and return to master: 
result <- mpi.remote.exec(Rcommand)

#Returns nslaves-length list

```

## Rmpi: Hello World

```{r,out.height="500px", out.width="750px", eval=T}
    knitr::include_graphics("Rmpi_hello_world.png",auto_pdf=F)
```

## Rmpi communications
- Broadcast a function or variable from mater to slave
    mpi.bcast.Robj2slave(object)
- Send object to destination
    mpi.send.Robj(object, destination, tag)
- Receive a sent message
    recv <- mpi.recv.Robj(mpi.any.source(),mpi.any.tag())
- Get tag from received message
    recv.info <- mpi.get.sourcetag()
    
    
## Rmpi Example: Pass messages

```{r eval=F, echo=T}

# Function to pass message to next slave
message.pass <- function() {
  # Get each slave's rank
  myrank <- mpi.comm.rank()
  # Get partner slave's rank (some hackery to avoid master)
  otherrank <- (myrank+1) %% mpi.comm.size()
  otherrank <- otherrank + (otherrank==0)
  # Send a message to the partner
  mpi.send.Robj(paste("I am rank",myrank), dest=otherrank, tag=myrank)
  # Receive the message & tag (includes source)
  recv.msg <- mpi.recv.Robj(mpi.any.source(),mpi.any.tag())
  recv.tag <- mpi.get.sourcetag()
  paste("Received message '",recv.msg,"' from process ",recv.tag[1],".
\n",sep="")
}

```



## Homework 7
